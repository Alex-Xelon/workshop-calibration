{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49769876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 : Import Libraries\n",
    "import marimo as mo\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy.io import arff\n",
    "from sklearn.metrics import log_loss, brier_score_loss, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from venn_abers import VennAbersCalibrator\n",
    "import calibration as cal\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bb0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 : Data Loading\n",
    "random.seed(1)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "\n",
    "data = ___  # TODO\n",
    "df = pd.DataFrame(data[0])\n",
    "\n",
    "print(___)  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1274ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 : Data preparation\n",
    "X = ___  # TODO\n",
    "y = ___  # TODO\n",
    "\n",
    "print(X.head(5))\n",
    "print(y.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188ceb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 : Data Splitting for Training, Calibration and Testing\n",
    "X_train, X_test, y_train, y_test = ___  # TODO\n",
    "X_proper_train, X_cal, y_proper_train, y_cal = ___  # TODO\n",
    "\n",
    "print(X_train.head(5))\n",
    "print(X_test.head(5))\n",
    "print(y_train.head(5))\n",
    "print(y_test.head(5))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(X_proper_train.head(5))\n",
    "print(X_cal.head(5))\n",
    "print(y_proper_train.head(5))\n",
    "print(y_cal.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69678866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 : Define the models to test\n",
    "clfs = {}\n",
    "clfs[\"SVM\"] = SVC(probability=True)\n",
    "clfs[\"RF\"] = RandomForestClassifier()\n",
    "clfs[\"AdaBoost\"] = AdaBoostClassifier()\n",
    "clfs[\"Logistic\"] = LogisticRegression(max_iter=10000)\n",
    "clfs[\"Neural Network\"] = MLPClassifier(max_iter=10000)\n",
    "\n",
    "for name_model in clfs.keys():\n",
    "    print(f\"- {name_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d165225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 : Example of calibration : Sigmoid and Isotonic\n",
    "model_example = ___  # TODO\n",
    "\n",
    "for method in [___]:  # TODO\n",
    "    print(f\"\\nCalibrating LogisticRegression with {method} method\")\n",
    "    # Wrap with CalibratedClassifierCV using the chosen method\n",
    "    calibrated_model = ___  # TODO\n",
    "\n",
    "    # Predict probabilities and classes on the test set\n",
    "    probs_cal = ___  # TODO\n",
    "    preds_cal = ___  # TODO\n",
    "    print(f\"Probs calibration: \\n{pd.DataFrame(probs_cal).head(10)}\")\n",
    "    print(f\"Preds calibration: \\n{pd.DataFrame(preds_cal).head(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022f2427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 : Example of metrics : Sigmoid and Isotonic\n",
    "acc_cal = ___  # TODO\n",
    "brier_cal = ___  # TODO\n",
    "logloss_cal = ___  # TODO\n",
    "ece_cal = ___  # TODO\n",
    "print(f\"Score Accuracy: {acc_cal:.3f}\")\n",
    "print(f\"Brier Score: {brier_cal:.3f}\")\n",
    "print(f\"Log Loss: {logloss_cal:.3f}\")\n",
    "print(f\"ECE: {ece_cal:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd23733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7 : Example of calibration : VennAbersCalibrator\n",
    "va = ___  # TODO\n",
    "\n",
    "probs_va = ___  # TODO\n",
    "preds_va = ___  # TODO\n",
    "print(f\"Probs calibration: \\n{pd.DataFrame(probs_va).head(10)}\")\n",
    "print(f\"Preds calibration: \\n{pd.DataFrame(preds_va).head(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70caca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8 : Example of metrics : VennAbersCalibrator\n",
    "acc_va = ___  # TODO\n",
    "brier_va = ___  # TODO\n",
    "logloss_va = ___  # TODO\n",
    "ece_va = ___  # TODO\n",
    "print(f\"Score Accuracy: {acc_va:.3f}\")\n",
    "print(f\"Brier Score: {brier_va:.3f}\")\n",
    "print(f\"Log Loss: {logloss_va:.3f}\")\n",
    "print(f\"ECE: {ece_va:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ec60a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9 : Define the metrics\n",
    "def metrics(\n",
    "    clf,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    acc_list,\n",
    "    log_loss_list,\n",
    "    brier_loss_list,\n",
    "    ece_list,\n",
    "    VennAbersCalibrator=False,\n",
    "):\n",
    "    if VennAbersCalibrator:\n",
    "        p_pred = ___  # TODO\n",
    "        y_pred = ___  # TODO\n",
    "    else:\n",
    "        p_pred = ___  # TODO\n",
    "        y_pred = ___  # TODO\n",
    "    acc_list.append(___)  # TODO\n",
    "    log_loss_list.append(___)  # TODO\n",
    "    brier_loss_list.append(___)  # TODO\n",
    "    ece_list.append(___)  # TODO\n",
    "    return acc_list, log_loss_list, brier_loss_list, ece_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1d82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Calibrate the models\n",
    "def run_multiclass_comparison(clf_name, clf):\n",
    "\n",
    "    print(clf_name + \":\")\n",
    "    log_loss_list = []\n",
    "    brier_loss_list = []\n",
    "    acc_list = []\n",
    "    ece_list = []\n",
    "\n",
    "    print(\"base\")\n",
    "    ___  # TODO\n",
    "    acc_list, log_loss_list, brier_loss_list, ece_list = ___  # TODO\n",
    "\n",
    "    print(\"ivap\")\n",
    "    va = ___  # TODO\n",
    "    ___  # TODO\n",
    "    acc_list, log_loss_list, brier_loss_list, ece_list = ___  # TODO\n",
    "\n",
    "    print(\"cvap\")\n",
    "    va_cv = ___  # TODO\n",
    "    ___  # TODO\n",
    "    acc_list, log_loss_list, brier_loss_list, ece_list = ___  # TODO\n",
    "\n",
    "    print(\"sigmoid_cv\")\n",
    "    cal_sigm_cv = ___  # TODO\n",
    "    ___  # TODO\n",
    "    acc_list, log_loss_list, brier_loss_list, ece_list = ___  # TODO\n",
    "\n",
    "    print(\"isotonic_cv\")\n",
    "    cal_iso_cv = ___  # TODO\n",
    "    ___  # TODO\n",
    "    acc_list, log_loss_list, brier_loss_list, ece_list = ___  # TODO\n",
    "\n",
    "    print(\"sigmoid\")\n",
    "    ___  # TODO\n",
    "    cal_sigm = ___  # TODO\n",
    "    ___  # TODO\n",
    "    acc_list, log_loss_list, brier_loss_list, ece_list = ___  # TODO\n",
    "\n",
    "    print(\"isotonic \\n\")\n",
    "    cal_iso = ___  # TODO\n",
    "    ___  # TODO\n",
    "    acc_list, log_loss_list, brier_loss_list, ece_list = ___  # TODO\n",
    "\n",
    "    df_ll = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"Classifier\",\n",
    "            \"Uncalibrated\",\n",
    "            \"Platt\",\n",
    "            \"Isotonic\",\n",
    "            \"Platt-CV\",\n",
    "            \"Isotonic-CV\",\n",
    "            \"IVAP\",\n",
    "            \"CVAP\",\n",
    "        ]\n",
    "    )\n",
    "    df_ll.loc[0] = [clf_name] + log_loss_list\n",
    "    df_bl = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"Classifier\",\n",
    "            \"Uncalibrated\",\n",
    "            \"Platt\",\n",
    "            \"Isotonic\",\n",
    "            \"Platt-CV\",\n",
    "            \"Isotonic-CV\",\n",
    "            \"IVAP\",\n",
    "            \"CVAP\",\n",
    "        ]\n",
    "    )\n",
    "    df_bl.loc[0] = [clf_name] + brier_loss_list\n",
    "    df_acc = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"Classifier\",\n",
    "            \"Uncalibrated\",\n",
    "            \"Platt\",\n",
    "            \"Isotonic\",\n",
    "            \"Platt-CV\",\n",
    "            \"Isotonic-CV\",\n",
    "            \"IVAP\",\n",
    "            \"CVAP\",\n",
    "        ]\n",
    "    )\n",
    "    df_acc.loc[0] = [clf_name] + acc_list\n",
    "    df_ece = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"Classifier\",\n",
    "            \"Uncalibrated\",\n",
    "            \"Platt\",\n",
    "            \"Isotonic\",\n",
    "            \"Platt-CV\",\n",
    "            \"Isotonic-CV\",\n",
    "            \"IVAP\",\n",
    "            \"CVAP\",\n",
    "        ]\n",
    "    )\n",
    "    df_ece.loc[0] = [clf_name] + ece_list\n",
    "\n",
    "    return df_bl, df_ll, df_acc, df_ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edf60cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11 : Compare models on multiclass classification\n",
    "print(\"Comparing models for multiclass classification\")\n",
    "results_brier = pd.DataFrame()\n",
    "results_log = pd.DataFrame()\n",
    "results_acc = pd.DataFrame()\n",
    "results_ece = pd.DataFrame()\n",
    "\n",
    "for ___ in ___:  # TODO\n",
    "    scratch_b, scratch_l, scratch_acc, scratch_ece = ___  # TODO\n",
    "    results_brier = ___  # TODO\n",
    "    results_log = ___  # TODO\n",
    "    results_acc = ___  # TODO\n",
    "    results_ece = ___  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbef273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12 : Define the function to convert the dataframe to a markdown table\n",
    "def df_to_markdown_table(df, higher_is_better=True):\n",
    "    # Convert to float and find best indices\n",
    "    df_float = df.select_dtypes(include=[\"number\"])\n",
    "    if higher_is_better:\n",
    "        best_indices = df_float.idxmax(axis=1)\n",
    "    else:\n",
    "        best_indices = df_float.idxmin(axis=1)\n",
    "\n",
    "    # Round and convert to string\n",
    "    formatted_df = df_float.round(4).astype(str)\n",
    "\n",
    "    # Bold the best values\n",
    "    for idx, best_col in enumerate(best_indices):\n",
    "        formatted_df.iloc[idx, formatted_df.columns.get_loc(best_col)] = (\n",
    "            f\"**{formatted_df.iloc[idx, formatted_df.columns.get_loc(best_col)]}**\"\n",
    "        )\n",
    "\n",
    "    # Generate markdown table\n",
    "    headers = [\"\"] + list(formatted_df.columns)\n",
    "    lines = [\"| \" + \" | \".join(headers) + \" |\"]\n",
    "    lines.append(\"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\")\n",
    "\n",
    "    for idx, row in formatted_df.iterrows():\n",
    "        line = \"| \" + str(idx) + \" | \" + \" | \".join(row.values) + \" |\"\n",
    "        lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def get_best_metric(df, metric_name, higher_is_better=True):\n",
    "    values = df.mean()\n",
    "    ranks = df.rank(axis=1, ascending=not higher_is_better).mean()\n",
    "    best_value = values.argmax() if higher_is_better else values.argmin()\n",
    "    best_rank = ranks.argmin()\n",
    "    formatted_values = values.round(4).astype(str)\n",
    "    formatted_values[values.index[best_value]] = (\n",
    "        f\"**{formatted_values[values.index[best_value]]}**\"\n",
    "    )\n",
    "    formatted_ranks = ranks.round(2).astype(str)\n",
    "    formatted_ranks[ranks.index[best_rank]] = (\n",
    "        f\"**{formatted_ranks[ranks.index[best_rank]]}**\"\n",
    "    )\n",
    "\n",
    "    # Create markdown table\n",
    "    headers = [\"Method\", \"Value\", \"Rank\"]\n",
    "    lines = [\"| \" + \" | \".join(headers) + \" |\"]\n",
    "    lines.append(\"| --- | --- | --- |\")\n",
    "\n",
    "    for method, val, rank in zip(values.index, formatted_values, formatted_ranks):\n",
    "        lines.append(f\"| {method} | {val} | {rank} |\")\n",
    "\n",
    "    return f\"### {metric_name} :\\n\" + \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "if \"Classifier\" in results_acc.columns:\n",
    "    results_acc.set_index(\"Classifier\", inplace=True)\n",
    "if \"Classifier\" in results_brier.columns:\n",
    "    results_brier.set_index(\"Classifier\", inplace=True)\n",
    "if \"Classifier\" in results_ece.columns:\n",
    "    results_ece.set_index(\"Classifier\", inplace=True)\n",
    "if \"Classifier\" in results_log.columns:\n",
    "    results_log.set_index(\"Classifier\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b2d5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13 : Display the results\n",
    "mo.md(\n",
    "    \"## Accuracy Results\\n\"\n",
    "    + df_to_markdown_table(results_acc, higher_is_better=True)\n",
    "    + \"\\n\\n## Brier Loss Results\\n\"\n",
    "    + df_to_markdown_table(results_brier, higher_is_better=False)\n",
    "    + \"\\n\\n## Log Loss Results\\n\"\n",
    "    + df_to_markdown_table(results_log, higher_is_better=False)\n",
    "    + \"\\n\\n## ECE Results\\n\"\n",
    "    + df_to_markdown_table(results_ece, higher_is_better=False)\n",
    "    + \"\\n\\n\\n## Summary Statistics\\n\"\n",
    "    + f\"{get_best_metric(results_acc, 'Accuracy', higher_is_better=True)}\\n\\n\"\n",
    "    + f\"{get_best_metric(results_brier, 'Brier Loss', higher_is_better=False)}\\n\\n\"\n",
    "    + f\"{get_best_metric(results_log, 'Log Loss', higher_is_better=False)}\\n\\n\"\n",
    "    + f\"{get_best_metric(results_ece, 'ECE', higher_is_better=False)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f94c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
